{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# 03: Custom Prior Distributions\n\n**Level:** Intermediate | **Time:** ~15 min\n\n## Overview\nLearn how to specify custom informative priors for Bayesian inference."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import sys\nsys.path.insert(0, '..')\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import truncnorm\nfrom src.config import CONFIG, get_theta_true\n\nnp.random.seed(42)\nprint('✓ Setup complete')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Uniform Priors (Default)\n\nBy default, all parameters use U(0,3) priors."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Default uniform prior\ntheta_uniform = np.random.uniform(0, 3, (1000, 5))\n\nfig, axes = plt.subplots(1, 5, figsize=(15, 3))\nfor i, ax in enumerate(axes):\n    ax.hist(theta_uniform[:, i], bins=30, density=True, alpha=0.7)\n    ax.set_title(f'θ[{i}] ~ U(0,3)')\n    ax.set_xlabel(f'θ[{i}]')\nplt.tight_layout()\nplt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Informative Gaussian Priors\n\nUse truncated normal when you have prior knowledge."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["def gaussian_prior(mean, std, lower, upper, size=1000):\n    a, b = (lower - mean) / std, (upper - mean) / std\n    return truncnorm.rvs(a, b, loc=mean, scale=std, size=size)\n\n# Example: Informative priors centered near truth\ntheta_true = get_theta_true()[0:5]\ntheta_informed = np.array([\n    gaussian_prior(0.8, 0.2, 0, 3, 1000),\n    gaussian_prior(2.0, 0.3, 0, 3, 1000),\n    gaussian_prior(1.0, 0.2, 0, 3, 1000),\n    gaussian_prior(0.1, 0.05, 0, 3, 1000),\n    gaussian_prior(0.2, 0.05, 0, 3, 1000)\n]).T\n\nfig, axes = plt.subplots(1, 5, figsize=(15, 3))\nfor i, ax in enumerate(axes):\n    ax.hist(theta_informed[:, i], bins=30, density=True, alpha=0.7, color='orange')\n    ax.axvline(theta_true[i], color='red', linestyle='--', label='True')\n    ax.set_title(f'θ[{i}] ~ N({theta_true[i]:.1f}, σ)')\n    ax.legend()\nplt.tight_layout()\nplt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Compare Prior Impact\n\nInformative priors lead to faster convergence and narrower posteriors."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Summary\n\n### When to Use Custom Priors\n- Previous studies provide parameter ranges\n- Physical constraints limit values\n- Expert knowledge available\n\n### Best Practices\n1. Don't make priors too narrow (avoid excluding truth)\n2. Use weakly informative priors when uncertain\n3. Validate sensitivity to prior choice\n\n### Next: `04_uncertainty_quantification.ipynb`"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
